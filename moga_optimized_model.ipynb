{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Genetic Algorithm Libraries\n",
    "from deap import base, creator, tools, algorithms\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-20T02:41:51.226540400Z",
     "start_time": "2024-12-20T02:41:48.344712200Z"
    }
   },
   "id": "c4ee83b799755d43"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-20T02:41:51.243504700Z",
     "start_time": "2024-12-20T02:41:51.226540400Z"
    }
   },
   "id": "41a765324691c38d"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class StrokeModelOptimizer:\n",
    "    def __init__(self, filepath):\n",
    "        \"\"\"\n",
    "        Initialize data, Genetic Algorithm (GA) settings, and constraints.\n",
    "        \"\"\"\n",
    "\n",
    "        # Load and preprocess data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test, self.scaler = self.load_and_preprocess_data(filepath)\n",
    "\n",
    "        # Enhanced GA Configuration\n",
    "        self.POPULATION_SIZE = 24\n",
    "        self.MAX_GENERATIONS = 10\n",
    "        self.CROSSOVER_PROB = 0.7\n",
    "        self.MUTATION_PROB = 0.4\n",
    "\n",
    "        # Gene configuration: ranges for hyperparameters\n",
    "        self.GENE_CONFIGS = {\n",
    "            'neurons_layer1': (32, 512),\n",
    "            'neurons_layer2': (16, 256),\n",
    "            'dropout_rate1': (0.1, 0.6),\n",
    "            'dropout_rate2': (0.1, 0.4),\n",
    "            'learning_rate': (0.0001, 0.01),\n",
    "            'l2_reg': (1e-4, 1)\n",
    "        }\n",
    "        self.TRAINING_EPOCHS = 20\n",
    "\n",
    "        # Tracking variables\n",
    "        self.generation_best_loss = []\n",
    "        self.generation_diversity = []\n",
    "        self.generation_individuals = []\n",
    "        self.generation_best_accuracy = []\n",
    "\n",
    "        # Global Optima\n",
    "        # Added a variable to track the absolute best individual across all generations\n",
    "        self.absolute_best_individual = None\n",
    "        self.absolute_best_fitness = (float('-inf'), float('inf'))  # Initialize with the worst possible fitness\n",
    "        \n",
    "        # Initialize Hall of Fame - CURRENTLY COMMENTED OUT TO REDUCE OUTPUT (already printing the best individuals in the output)\n",
    "        # self.hof = tools.HallOfFame(maxsize=10)  # Keep top 10 individuals\n",
    "\n",
    "        self.setup_deap_framework()\n",
    "\n",
    "    def load_and_preprocess_data(self, filepath):\n",
    "        \"\"\"Load dataset and preprocess.\"\"\"\n",
    "        df = pd.read_csv(filepath)\n",
    "        X = df.drop('stroke', axis=1)\n",
    "        y = df['stroke']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "\n",
    "    def calculate_weighted_sum(self, fitness_values):\n",
    "        \"\"\"Calculate weighted sum based on multi-objective weights\"\"\"\n",
    "        return fitness_values[0] * 1.0 + fitness_values[1] * -1.0\n",
    "\n",
    "    def weighted_tournament_selection(self, individuals, k, tournsize=5):\n",
    "        \"\"\"Tournament selection using weighted sum of objectives\"\"\"\n",
    "        selected = []\n",
    "        for _ in range(k):\n",
    "            tournament = random.sample(individuals, tournsize)\n",
    "            # Use weighted sum for comparison\n",
    "            winner = max(tournament,\n",
    "                         key=lambda ind: self.calculate_weighted_sum(ind.fitness.values))\n",
    "            selected.append(winner)\n",
    "        return selected\n",
    "\n",
    "    def setup_deap_framework(self):\n",
    "        \"\"\"Setup DEAP for GA optimization with proper multi-objective handling.\"\"\"\n",
    "        creator.create(\"FitnessMulti\", base.Fitness, weights=(1.0, -1.0))  # Maximize accuracy, Minimize loss\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "        self.toolbox = base.Toolbox()\n",
    "\n",
    "        # Gene initialization, generate a material attribute using a uniform distribution between defined values (constraints)\n",
    "        for gene, (low, high) in self.GENE_CONFIGS.items():\n",
    "            self.toolbox.register(f\"attr_{gene}\", random.uniform, low, high)\n",
    "\n",
    "        self.toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                              (self.toolbox.attr_neurons_layer1,\n",
    "                               self.toolbox.attr_neurons_layer2,\n",
    "                               self.toolbox.attr_dropout_rate1,\n",
    "                               self.toolbox.attr_dropout_rate2,\n",
    "                               self.toolbox.attr_learning_rate,\n",
    "                               self.toolbox.attr_l2_reg), n=1)\n",
    "        self.toolbox.register(\"population\", tools.initRepeat, list, self.toolbox.individual)\n",
    "        self.toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "        self.toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.5, indpb=0.3)\n",
    "        self.toolbox.register(\"select\", self.weighted_tournament_selection)\n",
    "        self.toolbox.register(\"evaluate\", self.evaluate_individual)\n",
    "\n",
    "    def build_ann(self, individual):\n",
    "        \"\"\"Build an ANN model based on individual hyperparameters.\"\"\"\n",
    "        neurons1, neurons2, drop1, drop2, lr, l2_reg = individual\n",
    "        model = Sequential([\n",
    "            Dense(int(neurons1), activation='relu', input_shape=(self.X_train.shape[1],),\n",
    "                  kernel_regularizer=l2(l2_reg)),\n",
    "            Dropout(drop1),\n",
    "            Dense(int(neurons2), activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "            Dropout(drop2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=lr), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def evaluate_individual(self, individual):\n",
    "        \"\"\"Evaluate the performance of an ANN defined by individual genes.\"\"\"\n",
    "        try:\n",
    "            model = self.build_ann(individual)  # Build model using individual's genes\n",
    "\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3,\n",
    "                                           restore_best_weights=True)  # Early stopping to prevent overfitting\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(self.X_train, self.y_train,\n",
    "                      epochs=self.TRAINING_EPOCHS,\n",
    "                      batch_size=64,\n",
    "                      validation_split=0.2,\n",
    "                      verbose=0,\n",
    "                      callbacks=[early_stopping])\n",
    "\n",
    "            # Evaluate on test data\n",
    "            loss, acc = model.evaluate(self.X_test, self.y_test, verbose=0)\n",
    "\n",
    "            # Clear model from memory\n",
    "            tf.keras.backend.clear_session()\n",
    "            del model\n",
    "\n",
    "            # Fitness: Maximize accuracy Minimize loss\n",
    "            return acc, loss\n",
    "        except Exception as e:\n",
    "            # print(f\"Evaluation error: {e}\")\n",
    "            return 0, 1e6  # Penalize invalid individuals\n",
    "\n",
    "    def is_individual_better(self, new_fitness, current_best_fitness):\n",
    "        \"\"\"Compare individuals using weighted sum approach\"\"\"\n",
    "        new_weighted = self.calculate_weighted_sum(new_fitness)\n",
    "        current_weighted = self.calculate_weighted_sum(current_best_fitness)\n",
    "        return new_weighted > current_weighted\n",
    "\n",
    "    def calculate_population_diversity(self, population):\n",
    "        \"\"\"Calculate population diversity based on gene variations\"\"\"\n",
    "        if not population:\n",
    "            return 0\n",
    "        gene_diversity = []\n",
    "        for gene_index in range(len(population[0])):\n",
    "            gene_values = [ind[gene_index] for ind in population]\n",
    "            gene_diversity.append(np.std(gene_values))\n",
    "        return np.mean(gene_diversity)\n",
    "\n",
    "    def run_optimization(self):\n",
    "        # Reset tracking variables\n",
    "        self.generation_best_loss = []\n",
    "        self.generation_diversity = []\n",
    "        self.generation_individuals = []\n",
    "        self.generation_best_accuracy = []\n",
    "\n",
    "        # Create initial population\n",
    "        pop = self.toolbox.population(n=self.POPULATION_SIZE)\n",
    "\n",
    "        # Evaluate initial population\n",
    "        fitnesses = list(map(self.toolbox.evaluate, pop))\n",
    "        for ind, fit in zip(pop, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "            # Check if this is the absolute best individual so far\n",
    "            if self.is_individual_better(fit, self.absolute_best_fitness):\n",
    "                self.absolute_best_individual = list(ind)\n",
    "                self.absolute_best_fitness = fit\n",
    "\n",
    "        # Update HoF with initial population - CURRENTLY COMMENTED OUT TO REDUCE OUTPUT (already printing the best individuals in the output)\n",
    "        # self.hof.update(pop)\n",
    "\n",
    "        # Generational loop with enhanced tracking and diversity preservation\n",
    "        for gen in range(self.MAX_GENERATIONS):\n",
    "            # Calculate and track population diversity\n",
    "            current_diversity = self.calculate_population_diversity(pop)\n",
    "            self.generation_diversity.append(current_diversity)\n",
    "\n",
    "            # Select next generation individuals using weighted selection\n",
    "            offspring = self.toolbox.select(pop, len(pop) - self\n",
    "                                            .POPULATION_SIZE // 10)  # Reduce size of offspring by 10%\n",
    "            # Implement Elitism to make sure best individuals are taken forward\n",
    "            elite = tools.selBest(pop, self.POPULATION_SIZE // 10)  # Select 10% of the population as elite\n",
    "            offspring.extend(elite)  # Add elite individuals to the offspring\n",
    "\n",
    "            offspring = list(map(self.toolbox.clone, offspring))\n",
    "\n",
    "            # Apply crossover and mutation\n",
    "            for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "                if random.random() < self.CROSSOVER_PROB:\n",
    "                    self.toolbox.mate(child1, child2)\n",
    "                    del child1.fitness.values\n",
    "                    del child2.fitness.values\n",
    "\n",
    "            for mutant in offspring:\n",
    "                if random.random() < self.MUTATION_PROB:\n",
    "                    self.toolbox.mutate(mutant)\n",
    "                    del mutant.fitness.values\n",
    "\n",
    "            # Evaluate individuals with invalid fitness\n",
    "            invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "            fitnesses = map(self.toolbox.evaluate, invalid_ind)\n",
    "            for ind, fit in zip(invalid_ind, fitnesses):\n",
    "                ind.fitness.values = fit\n",
    "\n",
    "                # Check if this is the absolute best individual so far\n",
    "                if self.is_individual_better(fit, self.absolute_best_fitness):\n",
    "                    self.absolute_best_individual = list(ind)\n",
    "                    self.absolute_best_fitness = fit\n",
    "\n",
    "            # # Update Hall of Fame - CURRENTLY COMMENTED OUT TO REDUCE OUTPUT (already printing the best individuals in the output)\n",
    "            # self.hof.update(offspring)\n",
    "\n",
    "            # Replace population\n",
    "            pop[:] = offspring\n",
    "\n",
    "            # Track generation statistics\n",
    "            best_ind = max(pop, key=lambda x: self.calculate_weighted_sum(x.fitness.values))\n",
    "            self.generation_best_loss.append(best_ind.fitness.values[1])\n",
    "            self.generation_best_accuracy.append(best_ind.fitness.values[0])\n",
    "\n",
    "            # Store information about each individual in this generation\n",
    "            self.generation_individuals.append([])\n",
    "            for ind in offspring:\n",
    "                ind_info = {\"individual\": ind, \"fitness\": ind.fitness.values}\n",
    "                self.generation_individuals[-1].append(ind_info)\n",
    "\n",
    "            # Print generation details\n",
    "            print(f\"\\nGeneration {gen + 1}:\")\n",
    "            print(f\"Diversity: {current_diversity}\")\n",
    "            print(\"\\nAll Individuals in this Generation:\")\n",
    "\n",
    "            for i, ind_info in enumerate(self.generation_individuals[-1], 1):\n",
    "                print(f\"Individual {i}: {ind_info['individual']}\")\n",
    "                print(f\"Fitness (Accuracy, Loss): {ind_info['fitness']}\\n\")\n",
    "\n",
    "            print(f\"Best Individual: {best_ind}\")\n",
    "            print(f\"Best Accuracy: {best_ind.fitness.values[0]}\")\n",
    "            print(f\"Best Loss: {best_ind.fitness.values[1]}\")\n",
    "            print(f\"Best Weighted Sum: {self.calculate_weighted_sum(best_ind.fitness.values)}\")\n",
    "\n",
    "            # # Print Hall of Fame information - CURRENTLY COMMENTED OUT TO REDUCE OUTPUT (already printing the best individuals in the output)\n",
    "            # print(\"\\nHall of Fame - Top 3 Individuals:\")\n",
    "            # for i, hof_ind in enumerate(self.hof[:3], 1):\n",
    "            #     print(f\"#{i} - Accuracy: {hof_ind.fitness.values[0]:.4f}, Loss: {hof_ind.fitness.values[1]:.4f}\")\n",
    "\n",
    "        # Store the best overall individual\n",
    "        self.best_individual = self.absolute_best_individual\n",
    "\n",
    "        # Visualize results\n",
    "        self.visualize_results(pop)\n",
    "\n",
    "        # def display_hall_of_fame(self): - CURRENTLY COMMENTED OUT TO REDUCE OUTPUT (already printing the best individuals in the output)\n",
    "        #     \"\"\"Display detailed information about Hall of Fame individuals\"\"\"\n",
    "        # \n",
    "        # print(\"\\nFinal Hall of Fame - All Individuals:\")\n",
    "        # for idx, individual in enumerate(self.hof, 1):\n",
    "        #     weighted_sum = self.calculate_weighted_sum(individual.fitness.values)\n",
    "        #     print(f\"\\nRank {idx}:\")\n",
    "        #     print(f\"Individual: {individual}\")\n",
    "        #     print(f\"Fitness (Accuracy, Loss): {individual.fitness.values}\")\n",
    "        #     print(f\"Weighted Sum: {weighted_sum}\")\n",
    "        #     print(\"Hyperparameters:\")\n",
    "        #     print(f\"Layer 1 Neurons: {int(individual[0])}\")\n",
    "        #     print(f\"Layer 2 Neurons: {int(individual[1])}\")\n",
    "        #     print(f\"Dropout Rate 1: {individual[2]:.3f}\")\n",
    "        #     print(f\"Dropout Rate 2: {individual[3]:.3f}\")\n",
    "        #     print(f\"Learning Rate: {individual[4]:.6f}\")\n",
    "        #     print(f\"L2 Regularization: {individual[5]:.6f}\")\n",
    "\n",
    "    def visualize_results(self, pop):\n",
    "\n",
    "        # Create a figure with multiple subplots\n",
    "        fig, axs = plt.subplots(3, 2, figsize=(15, 12))\n",
    "        # Adjust subplot spacing for better readability\n",
    "        plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.85, wspace=0.4, hspace=0.4)\n",
    "\n",
    "        # 1. Convergence Plot (Accuracy)\n",
    "        axs[0, 0].plot(range(1, len(self.generation_best_accuracy) + 1),\n",
    "                       self.generation_best_accuracy, marker='o', color='orange', label=\"Best Accuracy\")\n",
    "        axs[0, 0].set_title('Convergence Plot (Accuracy)')\n",
    "        axs[0, 0].set_xlabel('Generation')\n",
    "        axs[0, 0].set_ylabel('Accuracy')\n",
    "        axs[0, 0].legend()\n",
    "\n",
    "        # 2. Convergence Plot (Loss)\n",
    "        axs[0, 1].plot(range(1, len(self.generation_best_loss) + 1),\n",
    "                       self.generation_best_loss, marker='o', label=\"Best Loss\")\n",
    "        axs[0, 1].set_title('Convergence Plot (Loss)')\n",
    "        axs[0, 1].set_xlabel('Generation')\n",
    "        axs[0, 1].set_ylabel('Loss')\n",
    "        axs[0, 1].legend()\n",
    "\n",
    "        # 3. Pareto Front Visualization\n",
    "        accuracies = [ind.fitness.values[0] for ind in pop]\n",
    "        losses = [ind.fitness.values[1] for ind in pop]\n",
    "        axs[1, 0].scatter(losses, accuracies, c='b', label=\"Pareto Front\")\n",
    "        axs[1, 0].set_xlabel(\"Loss\")\n",
    "        axs[1, 0].set_ylabel(\"Accuracy\")\n",
    "        axs[1, 0].set_title(\"Pareto Front Visualization\")\n",
    "        axs[1, 0].legend()\n",
    "\n",
    "        # 4. Hyperparameter Distribution\n",
    "        hyperparameter_names = ['Neurons Layer 1', 'Neurons Layer 2',\n",
    "                                'Dropout Rate 1', 'Dropout Rate 2',\n",
    "                                'Learning Rate', 'L2 Regularization']\n",
    "        hyperparameter_values = list(zip(*pop))\n",
    "\n",
    "        for i in range(len(hyperparameter_names)):\n",
    "            axs[1, 1].hist(hyperparameter_values[i], bins=10,\n",
    "                           alpha=0.5, label=hyperparameter_names[i])\n",
    "        axs[1, 1].set_title('Hyperparameter Distribution')\n",
    "        axs[1, 1].set_xlabel('Value')\n",
    "        axs[1, 1].set_ylabel('Frequency')\n",
    "        axs[1, 1].legend()\n",
    "\n",
    "        # 5. Population Diversity Plot\n",
    "        axs[2, 0].plot(range(1, len(self.generation_diversity) + 1), self.generation_diversity, marker='o',\n",
    "                       color='green')\n",
    "        axs[2, 0].set_title('Population Diversity Over Generations')\n",
    "        axs[2, 0].set_xlabel('Generation')\n",
    "        axs[2, 0].set_ylabel('Diversity Metric')\n",
    "\n",
    "        # Remove the empty subplot\n",
    "        fig.delaxes(axs[2, 1])\n",
    "\n",
    "        # 6. Objective Space Evolution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for gen_idx, gen_individuals in enumerate(self.generation_individuals):\n",
    "            gen_accuracies = [ind['fitness'][0] for ind in gen_individuals]\n",
    "            gen_losses = [ind['fitness'][1] for ind in gen_individuals]\n",
    "            plt.scatter(gen_losses, gen_accuracies, alpha=0.5,\n",
    "                        label=f'Gen {gen_idx + 1}' if gen_idx % 3 == 0 else \"\")\n",
    "        plt.xlabel('Loss')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Evolution of Solutions in Objective Space')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # 7. Generation-wise Box Plots\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.ravel()\n",
    "\n",
    "        for idx, (name, values) in enumerate(zip(hyperparameter_names, zip(*pop))):\n",
    "            sns.boxplot(data=list(values), ax=axes[idx])\n",
    "            axes[idx].set_title(f'{name} Distribution')\n",
    "            axes[idx].tick_params(rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Best hyperparameters\n",
    "        print(\"\\nBest Overall Individual Hyperparameters:\", self.best_individual)\n",
    "        # Train with full training data using optimized hyperparameters\n",
    "        neurons1, neurons2, drop1, drop2, lr, l2_reg = self.best_individual\n",
    "        print(\"\\nTraining Final Model with Optimized Hyperparameters:\")\n",
    "        print(f\"Layer 1 Neurons: {int(neurons1)}\")\n",
    "        print(f\"Layer 2 Neurons: {int(neurons2)}\")\n",
    "        print(f\"Dropout Rate 1: {drop1}\")\n",
    "        print(f\"Dropout Rate 2: {drop2}\")\n",
    "        print(f\"Learning Rate: {lr}\")\n",
    "        print(f\"L2 Regularization: {l2_reg}\")\n",
    "\n",
    "        # Build and train final model using BEST optimized hyperparameters\n",
    "        final_model = self.build_ann(self.best_individual)\n",
    "        final_model.fit(self.X_train, self.y_train,\n",
    "                        epochs=5,\n",
    "                        batch_size=64,\n",
    "                        validation_split=0.2,\n",
    "                        verbose=1)\n",
    "\n",
    "        # Generate predictions and evaluation metrics\n",
    "        y_pred = (final_model.predict(self.X_test) > 0.5).astype(int)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred))\n",
    "\n",
    "        # Additional performance visualizations\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Confusion Matrix\n",
    "        plt.subplot(1, 3, 1)\n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "\n",
    "        # ROC Curve\n",
    "        plt.subplot(1, 3, 2)\n",
    "        fpr, tpr, _ = roc_curve(self.y_test, final_model.predict(self.X_test))\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"ROC Curve\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Precision-Recall Curve\n",
    "        plt.subplot(1, 3, 3)\n",
    "        precision, recall, _ = precision_recall_curve(self.y_test, final_model.predict(self.X_test))\n",
    "        plt.plot(recall, precision)\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.title(\"Precision-Recall Curve\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-20T02:42:29.482784Z",
     "start_time": "2024-12-20T02:42:29.420762400Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Generation 1:\n",
      "Diversity: 36.93484416582657\n",
      "\n",
      "All Individuals in this Generation:\n",
      "Individual 1: [187.37966988715078, 100.94763236891163, 1.2371109077130167, 0.2105451869045245, -0.00274755010390242, 0.5317083608120048]\n",
      "Fitness (Accuracy, Loss): (0.0, 1000000.0)\n",
      "\n",
      "Individual 2: [488.9890332871209, 59.212041220471065, 0.5701841998603983, 0.2610022715166142, 0.012901574422854362, -0.025590729622672677]\n",
      "Fitness (Accuracy, Loss): (0.0, 1000000.0)\n",
      "\n",
      "Individual 3: [396.0082678211914, 192.63433950003594, -0.7115439087757571, -0.45159928826518103, 0.006611572637675859, 0.06046217401291258]\n",
      "Fitness (Accuracy, Loss): (0.0, 1000000.0)\n",
      "\n",
      "Individual 4: [75.81876215327573, 210.7940689708912, 0.5285032373397403, 0.37926209743737105, 0.003608078382338233, 0.04347700118192221]\n",
      "Fitness (Accuracy, Loss): (0.8893616795539856, 0.3256392180919647)\n",
      "\n",
      "Individual 5: [210.40895728995753, 197.5229835898234, 0.5228650943310513, 0.15918580149528, 0.004063636228177211, 0.45599548951968993]\n",
      "Fitness (Accuracy, Loss): (0.8856382966041565, 0.4502333402633667)\n",
      "\n",
      "Individual 6: [124.01000431637792, 213.97988902397742, 0.5111010580395277, 0.24647890376995246, 0.001721090581925119, 0.10441965040027448]\n",
      "Fitness (Accuracy, Loss): (0.8898935914039612, 0.3554666340351105)\n",
      "\n",
      "Individual 7: [41.3488363451995, 239.49815509437582, 0.5393609389115921, 0.3494996588083539, 0.003144389841486348, 0.05801937397753814]\n",
      "Fitness (Accuracy, Loss): (0.8877659440040588, 0.3255467712879181)\n",
      "\n",
      "Individual 8: [470.06136288551386, 224.92445676082406, 0.24922239572431645, 0.2916848484598016, 0.006128805093237906, 0.15292398462277984]\n",
      "Fitness (Accuracy, Loss): (0.8813830018043518, 0.3727567493915558)\n",
      "\n",
      "Individual 9: [125.17383548249416, 174.5720929348547, 0.5322668263054573, 0.33367954015145246, 0.00558061432793466, 0.1902261133187992]\n",
      "Fitness (Accuracy, Loss): (0.882446825504303, 0.4111570119857788)\n",
      "\n",
      "Individual 10: [346.28923970414286, 228.85631553607243, 0.5379475627615236, 0.28902576223393006, 0.004639036692079432, 0.061037315322764775]\n",
      "Fitness (Accuracy, Loss): (0.88882976770401, 0.36312544345855713)\n",
      "\n",
      "Individual 11: [41.3488363451995, 238.9836679035081, 0.5393609389115921, 0.3494996588083539, 0.003144389841486348, 0.05801937397753814]\n",
      "Fitness (Accuracy, Loss): (0.8936170339584351, 0.324691504240036)\n",
      "\n",
      "Individual 12: [41.3488363451995, 238.9836679035081, 0.5393609389115921, 0.3494996588083539, 0.003144389841486348, 0.05801937397753814]\n",
      "Fitness (Accuracy, Loss): (0.8936170339584351, 0.324691504240036)\n",
      "\n",
      "Individual 13: [223.7122424673907, 68.636982197748, 0.35007379436977804, 0.2528578881029394, -0.41909507543359914, 0.0472116637871921]\n",
      "Fitness (Accuracy, Loss): (0.17606383562088013, 2715292928.0)\n",
      "\n",
      "Individual 14: [430.11423884143755, 164.44474056741905, 0.5308534501553887, 0.27320564357702865, 0.007075261178527743, 0.04591980121729665]\n",
      "Fitness (Accuracy, Loss): (0.8851063847541809, 0.35954684019088745)\n",
      "\n",
      "Individual 15: [491.462274659255, 96.78269082703042, 0.14637292169007396, 0.1290149130500392, 0.008490194226839852, 0.6037656587637544]\n",
      "Fitness (Accuracy, Loss): (0.882446825504303, 0.4192787706851959)\n",
      "\n",
      "Individual 16: [223.86441933099783, 68.636982197748, 0.5987688032475551, 0.2528578881029394, 0.0010000031805205596, 0.0472116637871921]\n",
      "Fitness (Accuracy, Loss): (0.8946808576583862, 0.3279280662536621)\n",
      "\n",
      "Individual 17: [545.0845398738816, 240.38641247699303, 0.20689255771589754, 0.31045088957027145, 0.006550439320174535, 0.0249000763426577]\n",
      "Fitness (Accuracy, Loss): (0.8930851221084595, 0.30001911520957947)\n",
      "\n",
      "Individual 18: [-33.67434064316812, 223.52171218733915, 0.5816907769200109, 0.33073361769788406, 0.0027227556145497183, 0.1860432822576603]\n",
      "Fitness (Accuracy, Loss): (0.0, 1000000.0)\n",
      "\n",
      "Individual 19: [41.3488363451995, 238.9836679035081, 0.5393609389115921, 0.3494996588083539, 0.003144389841486348, 0.05801937397753814]\n",
      "Fitness (Accuracy, Loss): (0.8936170339584351, 0.324691504240036)\n",
      "\n",
      "Individual 20: [470.06136288551386, 224.92445676082406, -0.41664895004469926, 0.2916848484598016, -0.32972664144339464, 0.9947093177040063]\n",
      "Fitness (Accuracy, Loss): (0.0, 1000000.0)\n",
      "\n",
      "Individual 21: [77.6983037737166, 168.75061731251287, 0.539519733747747, 0.3718100518435376, 0.0009642960071243303, 0.14381674590415705]\n",
      "Fitness (Accuracy, Loss): (0.8851063847541809, 0.3863145411014557)\n",
      "\n",
      "Individual 22: [48.28211513979929, 236.82010059973683, 0.49588088734532715, 0.20433759700472154, 0.0029090181252663934, 0.2958837526512672]\n",
      "Fitness (Accuracy, Loss): (0.882446825504303, 0.42564505338668823)\n",
      "\n",
      "Individual 23: [143.2206745825519, 51.0532826137785, 0.5193462035208383, 0.2721202213112701, 0.0034469043904227854, -0.9156093182300657]\n",
      "Fitness (Accuracy, Loss): (0.0, 1000000.0)\n",
      "\n",
      "Individual 24: [121.84040423003829, 256.16387045277645, 0.6187835386383088, 0.33023732560002317, 0.0006974886315841223, 0.0445892153651514]\n",
      "Fitness (Accuracy, Loss): (0.8898935914039612, 0.3252100944519043)\n",
      "\n",
      "Best Individual: [545.0845398738816, 240.38641247699303, 0.20689255771589754, 0.31045088957027145, 0.006550439320174535, 0.0249000763426577]\n",
      "Best Accuracy: 0.8930851221084595\n",
      "Best Loss: 0.30001911520957947\n",
      "Best Weighted Sum: 0.59306600689888\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m filepath \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData/cleaned-stroke-prediction-dataset-balanced.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      4\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m StrokeModelOptimizer(filepath)\n\u001B[1;32m----> 5\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_optimization\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[3], line 210\u001B[0m, in \u001B[0;36mStrokeModelOptimizer.run_optimization\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    208\u001B[0m invalid_ind \u001B[38;5;241m=\u001B[39m [ind \u001B[38;5;28;01mfor\u001B[39;00m ind \u001B[38;5;129;01min\u001B[39;00m offspring \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ind\u001B[38;5;241m.\u001B[39mfitness\u001B[38;5;241m.\u001B[39mvalid]\n\u001B[0;32m    209\u001B[0m fitnesses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoolbox\u001B[38;5;241m.\u001B[39mevaluate, invalid_ind)\n\u001B[1;32m--> 210\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ind, fit \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(invalid_ind, fitnesses):\n\u001B[0;32m    211\u001B[0m     ind\u001B[38;5;241m.\u001B[39mfitness\u001B[38;5;241m.\u001B[39mvalues \u001B[38;5;241m=\u001B[39m fit\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;66;03m# Check if this is the absolute best individual so far\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[3], line 121\u001B[0m, in \u001B[0;36mStrokeModelOptimizer.evaluate_individual\u001B[1;34m(self, individual)\u001B[0m\n\u001B[0;32m    117\u001B[0m early_stopping \u001B[38;5;241m=\u001B[39m EarlyStopping(monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[0;32m    118\u001B[0m                                restore_best_weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)  \u001B[38;5;66;03m# Early stopping to prevent overfitting\u001B[39;00m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m--> 121\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[43m          \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTRAINING_EPOCHS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    123\u001B[0m \u001B[43m          \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    124\u001B[0m \u001B[43m          \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[43m          \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    126\u001B[0m \u001B[43m          \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;66;03m# Evaluate on test data\u001B[39;00m\n\u001B[0;32m    129\u001B[0m loss, acc \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mevaluate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mX_test, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_test, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:368\u001B[0m, in \u001B[0;36mTensorFlowTrainer.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[0m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, iterator \u001B[38;5;129;01min\u001B[39;00m epoch_iterator:\n\u001B[0;32m    367\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m--> 368\u001B[0m     logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    369\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_end(step, logs)\n\u001B[0;32m    370\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:216\u001B[0m, in \u001B[0;36mTensorFlowTrainer._make_function.<locals>.function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunction\u001B[39m(iterator):\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[0;32m    214\u001B[0m         iterator, (tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mIterator, tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mDistributedIterator)\n\u001B[0;32m    215\u001B[0m     ):\n\u001B[1;32m--> 216\u001B[0m         opt_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmulti_step_on_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m opt_outputs\u001B[38;5;241m.\u001B[39mhas_value():\n\u001B[0;32m    218\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    830\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 833\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    835\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    836\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    875\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    876\u001B[0m \u001B[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001B[39;00m\n\u001B[0;32m    877\u001B[0m \u001B[38;5;66;03m# run the first trace but we should fail if variables are created.\u001B[39;00m\n\u001B[1;32m--> 878\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mtracing_compilation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    879\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_variable_creation_config\u001B[49m\n\u001B[0;32m    880\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    881\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_created_variables:\n\u001B[0;32m    882\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating variables on a non-first call to a function\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    883\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m decorated with tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001B[0m, in \u001B[0;36mcall_function\u001B[1;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[0;32m    137\u001B[0m bound_args \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mbind(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    138\u001B[0m flat_inputs \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39munpack_inputs(bound_args)\n\u001B[1;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[0;32m    140\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\n\u001B[0;32m    141\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, tensor_inputs, captured_inputs)\u001B[0m\n\u001B[0;32m   1318\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1320\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1321\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1322\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_preflattened\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1323\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1324\u001B[0m     args,\n\u001B[0;32m   1325\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1326\u001B[0m     executing_eagerly)\n\u001B[0;32m   1327\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001B[0m, in \u001B[0;36mAtomicFunction.call_preflattened\u001B[1;34m(self, args)\u001B[0m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall_preflattened\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: Sequence[core\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    215\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 216\u001B[0m   flat_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mpack_output(flat_outputs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001B[0m, in \u001B[0;36mAtomicFunction.call_flat\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m record\u001B[38;5;241m.\u001B[39mstop_recording():\n\u001B[0;32m    250\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[1;32m--> 251\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bound_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction_type\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflat_outputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    256\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    257\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m make_call_op_in_graph(\n\u001B[0;32m    258\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    259\u001B[0m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[0;32m    260\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mfunction_call_options\u001B[38;5;241m.\u001B[39mas_attrs(),\n\u001B[0;32m    261\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001B[0m, in \u001B[0;36mContext.call_function\u001B[1;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[0;32m   1681\u001B[0m cancellation_context \u001B[38;5;241m=\u001B[39m cancellation\u001B[38;5;241m.\u001B[39mcontext()\n\u001B[0;32m   1682\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1683\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1684\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1685\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1686\u001B[0m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1687\u001B[0m \u001B[43m      \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1688\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1689\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1690\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1691\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m   1692\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   1693\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1697\u001B[0m       cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_context,\n\u001B[0;32m   1698\u001B[0m   )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     52\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 53\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     56\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Run the optimizer\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = r\"Data/cleaned-stroke-prediction-dataset-balanced.csv\"\n",
    "    optimizer = StrokeModelOptimizer(filepath)\n",
    "    optimizer.run_optimization()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-20T02:56:31.476217700Z",
     "start_time": "2024-12-20T02:42:30.905830400Z"
    }
   },
   "id": "db15ab840fb1fee7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce227be730adfd4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
